{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfbcacc",
   "metadata": {},
   "source": [
    "## What is artificial intelligence?\n",
    "\n",
    "Usually due to lack of knowledge, we usually call artificial intelligence to any operation that is carried out by a computer. However, this is far from the truth. Artificial intelligence is nourished by different areas, the most popular: data science, deep learning and deep learning.\n",
    "\n",
    "Let's be clear about something, data science is not artificial intelligence, the opposite is not true either, however, you can solve a problem with data science without touching artificial intelligence at any time, the opposite cannot always be carried out.\n",
    "\n",
    "Let's first understand the following diagram:\n",
    "\n",
    "<img src=\"./assets/images/components.png\"/>\n",
    "\n",
    "Natural language processing, image recognition, the process of going from text to speech, language translation, among other areas, are what together we should call artificial intelligence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e52ee5",
   "metadata": {},
   "source": [
    "## When do we have an AI problem?\n",
    "As we have indicated, it is possible to solve a computational problem simply using data science, applying for example an excellent visualization of the data. We will have an AI problem in cases where we want a machine to carry out very specific tasks in an automated way, where using data we can get that machine to carry out the work for which it was programmed in a correct way and with high precision.\n",
    "\n",
    "\n",
    "## Areas where Artificial Intelligence can be applied\n",
    "Nowadays, there are few areas where algorithms that are related to artificial intelligence are not carried out, from very repetitive tasks such as tightening thousands of screws in one piece to areas where the human is not able to overcome these algorithms such as disease identification in X-ray or microscopic images.\n",
    "\n",
    "At visionanalytics.ai, during the 2020 Madrid strong lockdown caused by the COVID-19 pandemic, we developed an algorithm capable of differentiating X-ray images of patients from images of patients infected with pneumonia caused by COVID-19, in addition From this, the AI was able to pinpoint the affected area:\n",
    "\n",
    "<img src=\"./assets/images/covid.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Other solutions were also implemented to reduce the risk of contagion by COVID-19 in different public spaces with a mask detector:\n",
    "\n",
    "<img src=\"./assets/images/mask.png\" width=\"500\"/>\n",
    "\n",
    "The applications as you can see are innumerable, such as increasing the security of spaces by detecting guns:\n",
    "\n",
    "<img src=\"./assets/images/gun.png\" width=\"500\"/>\n",
    "\n",
    "In fact, we use a part of artificial intelligence, every time we take a ‚Äúselfie‚Äù with our mobile phone and it automatically recognizes our face\n",
    "\n",
    "<img src=\"./assets/images/phone.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "O cuando usamos un filtro de instagram. El dispositivo reconoce la cara de la persona y coloca un ‚Äúdibujo‚Äù en las partes adecuadas de la cara. En este caso, no solo reconoce una cara, sino adem√°s detecta la posici√≥n de cada una de las partes que la componen\n",
    "\n",
    "\n",
    "<img src=\"./assets/images/instagram.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40631cd6",
   "metadata": {},
   "source": [
    "## Data Science vs Machine Learning vs Deep Learning vs Artificial Intelligence\n",
    "\n",
    "It usually confuses all these terms but the reality is that although they go hand in hand, many times they do not even require one of the other.\n",
    "\n",
    "Something similar happens when we talk about Machine Learning and Artificial Intelligence, where AI is composed (among other things) of different machine learning algorithms.\n",
    "\n",
    "On the othe hand, Machine learning is the process of data analysis using an algorithm or statistical model that ‚Äúlearns‚Äù based on patterns within a model dataset it is exposed to. And just as a curious fact, this algorithm was introduced for the first time in 1847 by the French mathematician Augustin Louis Cauchy as a method of solving non-linear equations. And yes, the artificial intelligence we know today is based on an algorithm from 1847.\n",
    "\n",
    "Each new dataset the algorithm is exposed to helps to ‚Äútrain‚Äù it to achieve a certain outcome, as it adjusts its calculation and decision-making process.\n",
    "\n",
    "Therefore, we have what is seen in the image:\n",
    "\n",
    "<img src=\"./assets/images/diagram.png\" alt=\"diagram\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5db19b",
   "metadata": {},
   "source": [
    "## And finally, Big Data.\n",
    "\n",
    "Data Science is any operation that requires data management. From a simple calculation of the age mean using statistics, to complicated data transformations using relational operations (SQL). And this is where the first confusion is generated, so what is Big Data? Well, Big Data, a term very badly used in our days like artificial intelligence, is nothing more than massive data processing, that is, let's say that in data science we process data of any type and at any scale, while the Big Data is the term used to do this processing but on a large scale. Therefore, Big Data is only a small part of what is done in Data Science, although it certainly also requires the help of other professionals such as Data Engineers or Data Architect who support that great computing capacity that data science requires. \n",
    "\n",
    "\n",
    "### So how are data science and Big Data related to machine learning and artificial intelligence?\n",
    "\n",
    "Well, to train these very precise models, large amounts of data are required and this is where the work of the data scientist and data engineer comes in, taking the previous diagram to the next step:\n",
    "<img src=\"./assets/images/ds_big_data.png\" alt=\"diagram\" width=\"500\"/>\n",
    "\n",
    "\n",
    "With this introduction, we proceed to solve the next notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c20b3d",
   "metadata": {},
   "source": [
    "## What we will learn in this notebook?\n",
    "\n",
    "- Types of data.\n",
    "- Metricas estadisticas.\n",
    "- Quantile vs Percentile.\n",
    "- Data visualization: One variable.\n",
    "- Data visualization: Two variable.\n",
    "- Data visualization: Three variable.\n",
    "\n",
    "## What is the goal of this notebook?\n",
    "\n",
    "Although in the other lessons we have learned to deal with real data, in this notebook we will define exactly and put into context fundamental concepts in the area of data science so that you can carry out a preliminary analysis of the data to then adjust a machine learning model that we will learn throughout the bootcamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6edd5c1",
   "metadata": {},
   "source": [
    "## Types of variable\n",
    "\n",
    "#### Exercise: What is a data?\n",
    "We can define that a data is any vector that we can read with a computer. So, under this definition, everything we mention below is a data:\n",
    "\n",
    "- Images\n",
    "- Words\n",
    "- Audios\n",
    "- Tables\n",
    "- Vectors\n",
    "\n",
    "In any case, We always seek to bring everything to a simple vector or array. But actually, how can an image be an array?\n",
    "\n",
    "<img src=\"./assets/images/im_array.png\" alt=\"diagram\" width=\"600\"/>\n",
    "\n",
    "An image is just a composition of three channels: Red, Green, Blue (RGB) with colors from 0 to 255 (256 colors). So, as you can see, you just have to bring each data to an array and that's why `numpy`is so important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe8a06",
   "metadata": {},
   "source": [
    "#### Exercise: Read the image \"4geeks_black.jpg\" and get the dimensions of the array (‚òÖ‚òÜ‚òÜ)\n",
    "Use `cv2.imread(path_to_image)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a758bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a708990",
   "metadata": {},
   "source": [
    "#### Exercise: Plot the image using matplotlib (‚òÖ‚òÜ‚òÜ)\n",
    "You can use `plt.imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e01f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b16cd77",
   "metadata": {},
   "source": [
    "#### Exercise: Now try the same (read and plot) with the image \"4geeks_blue.png\" (‚òÖ‚òÖ‚òÜ) \n",
    "Is the image blue? Does it look exactly how it should be? If not, try to analyze why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b5975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a66543ce",
   "metadata": {},
   "source": [
    " #### Exercise: Read and plot the image \"4geeks_blue.png\" using just matplotlib (‚òÖ‚òÖ‚òÜ)\n",
    " Use `plt.imread` to read the image and `plt.imshow`to plot the image.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af17c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "782d4c77",
   "metadata": {},
   "source": [
    "#### Exercise: Change the order of the array using numpy (‚òÖ‚òÖ‚òÜ)\n",
    "Please try `[:,:,::-1]` and response why didn't we have that problem with the 4geeks_black image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe1a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48d48f01",
   "metadata": {},
   "source": [
    "#### Exercise: Now change the order of the channels using openCV (‚òÖ‚òÖ‚òÜ)\n",
    "Use `cv2.cvtColor` and `cv2.COLOR_BGR2RGB`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed233e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c06e584d",
   "metadata": {},
   "source": [
    "## The library Scikit-Learn\n",
    "\n",
    "<img src=\"./assets/images/sklearn_logo.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Scikit-Learn is one of these free libraries for Python. It has algorithms for classification, regression, clustering, and dimensionality reduction. In addition, it presents the compatibility with other Python libraries such as NumPy, SciPy and matplotlib.\n",
    "\n",
    "Scikit-learn's wide variety of algorithms and utilities make it the basic tool to start programming and structuring data analysis and statistical modeling systems. Scikit-Learn algorithms are combined and debugged with other data structures and external applications such as Pandas.\n",
    "\n",
    "We will use this series later in the bootcamp, but for the moment what interests us is to use the preloaded datasets that the library has.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b1cbbd",
   "metadata": {},
   "source": [
    "#### Load the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa25902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "# save load_iris() sklearn dataset to iris\n",
    "# if you'd like to check dataset type use: type(load_iris())\n",
    "# if you'd like to view list of attributes use: dir(load_iris())\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# np.c_ is the numpy concatenate function\n",
    "# which is used to concat iris['data'] and iris['target'] arrays \n",
    "# for pandas column argument: concat iris['feature_names'] list\n",
    "# and string list (in this case one string); you can make this anything you'd like..  \n",
    "# the original dataset would probably call this ['Species']\n",
    "iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa39382",
   "metadata": {},
   "source": [
    "#### Exercise: Make a description of the dataset (‚òÖ‚òÜ‚òÜ)\n",
    "Remember the function `pd.describe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ad239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7bdb78",
   "metadata": {},
   "source": [
    "#### The iris dataset consists of 3 different types of irises (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width. https://en.wikipedia.org/wiki/Iris_flower_data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64551a",
   "metadata": {},
   "source": [
    "## Statistics is the science that studies how information should be used and then respond to practical situations that involve uncertainty.\n",
    "\n",
    "Statistics deals with obtaining research conclusions through the use of mathematical models, providing a methodology through which the discrepancies between what is observed and what is predicted by the model can be evaluated and judged (inference), all this based on information contained in a data set.\n",
    "\n",
    "## Statistical inference vs descriptive statistics\n",
    "\n",
    "<img src=\"./assets/images/pop_samp.png\" alt=\"diagram\" width=\"600\"/>\n",
    "\n",
    "Descriptive Statistics provides the techniques to summarize and present the information extracted from a sample. However, we are rarely interested in the sample as such, but rather because of its ability to provide information about other subjects or other situations.\n",
    "Inferential Statistics provides the techniques to draw conclusions from a sample.\n",
    "\n",
    "## Basic metrics for descriptive statistics\n",
    "\n",
    "- Mean\n",
    "- Median\n",
    "- Variance\n",
    "- Standard deviation\n",
    "- Quantiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f989e6d",
   "metadata": {},
   "source": [
    "## Definitions:\n",
    "\n",
    "- Mean: The sample mean, also called the sample arithmetic mean or simply the average, is the arithmetic average of all the items in a dataset. The mean of a dataset ùë• is mathematically expressed as $\\sum_i \\frac{x_i}{n}$, where $ùëñ = 1, 2, \\ldots, n$ . In other words, it‚Äôs the sum of all the elements ùë•·µ¢ divided by the number of items in the dataset $x$.\n",
    "\n",
    "- Median: The sample median is the middle element of a sorted dataset. The dataset can be sorted in increasing or decreasing order. If the number of elements ùëõ of the dataset is odd, then the median is the value at the middle position: $0.5(n + 1)$. If $n$ is even, then the median is the arithmetic mean of the two values in the middle, that is, the items at the positions $0.5n$ and $0.5ùëõ + 1$.\n",
    "\n",
    "- Variance: The sample variance quantifies the spread of the data. It shows numerically how far the data points are from the mean. You can express the sample variance of the dataset $x$ with $n$ elements mathematically as $s^2 = \\sum_i \\frac{(x_i - \\bar{x})^2}{n-1}$, where $i = 1, 2, \\ldots, n$ and $\\bar{x}$ is the sample mean of $x$. We will undestand deeper If you want to understand deeper why you divide the sum with $n ‚àí 1$ instead of $n$ in the bootcamp.\n",
    "\n",
    "\n",
    "\n",
    "- Standard deviation: The sample standard deviation is another measure of data spread. It‚Äôs connected to the sample variance, as standard deviation, $s$, is the positive square root of the sample variance. The standard deviation is often more convenient than the variance because it has the same unit as the data points. Once you get the variance, you can calculate the standard deviation with pure Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab5f03",
   "metadata": {},
   "source": [
    "## Percentiles\n",
    "\n",
    "The sample $p$ percentile is the element in the dataset such that $p\\%$ of the elements in the dataset are less than or equal to that value. Also, $(100 ‚àí p)\\%$ of the elements are greater than or equal to that value. If there are two such elements in the dataset, then the sample ùëù percentile is their arithmetic mean. Each dataset has three quartiles, which are the percentiles that divide the dataset into four parts:\n",
    "\n",
    "The first quartile is the sample 25th percentile. It divides roughly 25% of the smallest items from the rest of the dataset.\n",
    "The second quartile is the sample 50th percentile or the median. Approximately 25% of the items lie between the first and second quartiles and another 25% between the second and third quartiles.\n",
    "The third quartile is the sample 75th percentile. It divides roughly 25% of the largest items from the rest of the dataset.\n",
    "\n",
    "## Quantiles\n",
    "\n",
    "What's the difference with between quantiles and percentiles? \n",
    "\n",
    "The definition is almost the same, only in the case of quantiles, we seek to divide the sample into 5 parts, we start with the $q20$ which means it  divides roughly 20% of the smallest items from the rest of the dataset and we end with $q80$ which means it divides roughly 80% of the largest items from the rest of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99137c2c",
   "metadata": {},
   "source": [
    "## The statistics library\n",
    "\n",
    "The statistics library is provided by Python. It offers functions for the calculation of statistical values in the field of real numbers. Some of the functions it offers are:\n",
    "\n",
    "- `statistics.mean()`: returns the arithmetic mean of the data:\n",
    "- `statistics.median()`: returns the median of the data:\n",
    "- `statistics.mode()`: returns the mode of the data:\n",
    "- `statistics.stdev()`: returns the sample standard deviation of the population represented by the data\n",
    "- `statistics.variance()`: returns the sample variance of the population represented by the data\n",
    "- `statistics.pstdev()`: returns the standard deviation of the population represented by the data\n",
    "- `statistics.pvariance()`: returns the variance of the population represented by the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f76de",
   "metadata": {},
   "source": [
    "#### Exercise: Calculate the mean, median, variance and standar deviation of each columf for the dataset iris using two different methods (‚òÖ‚òÖ‚òÜ) \n",
    "Hint: Use `np.sum` or `statistics.mean`. Remember yuou can apply a function to the entire dataset using pandas. Check the pandas notebook if you don't remember how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f7adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89b548e3",
   "metadata": {},
   "source": [
    "#### Exercise: Calculate all percentiles and all quantiles using two different methods (‚òÖ‚òÖ‚òÜ) \n",
    "Hint: Use `numpy`and `statistics`libraries. Remember yuou can apply a function to the entire dataset using pandas. Check the pandas notebook if you don't remember how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba51bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5056596",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "Unquestionably, the world we live in is a three-dimensional space. So that means that at most, we can do a data visualization like the one we see below:\n",
    "<img src=\"./assets/images/3d.png\" alt=\"diagram\"/>\n",
    "\n",
    "But what if we tell you we can make 4d data visualization? and what about 5d data visualization?\n",
    "\n",
    "that's exactly what we will do next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86dc493",
   "metadata": {},
   "source": [
    "#### This is also a 3D plot: Run the following lines of code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5280d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import some data to play with\n",
    "X = iris.iloc[:,0:2]  # we only take the first two features.\n",
    "y = iris.target\n",
    "\n",
    "x_min, x_max = X.iloc[:, 0].min() - 0.5, X.iloc[:, 0].max() + 0.5\n",
    "y_min, y_max = X.iloc[:, 1].min() - 0.5, X.iloc[:, 1].max() + 0.5\n",
    "\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"Sepal length\")\n",
    "plt.ylabel(\"Sepal width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfaa23",
   "metadata": {},
   "source": [
    "#### Repeat the previous exercise but with petal variables (‚òÖ‚òÖ‚òÖ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5c7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff286887",
   "metadata": {},
   "source": [
    "#### Now we will do something similar but with 3D plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f842bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "X = iris.iloc[:,0:3]\n",
    "\n",
    "ax.scatter(X.iloc[:,0], X.iloc[:,1], X.iloc[:,2], c = y)\n",
    "ax.set_xlabel('Sepal length')\n",
    "ax.set_ylabel('Sepal width')\n",
    "ax.set_zlabel('Petal length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a8a8f",
   "metadata": {},
   "source": [
    "#### Repeat the previous exercise but with the following variables sepal width, petal length, petal width (cm) (‚òÖ‚òÖ‚òÜ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b25425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "090a6a3e",
   "metadata": {},
   "source": [
    "#### As you can see, we can change the colors of the scatter plot points and \"add\" an extra dimension to our plot and better understand our data\n",
    "\n",
    "#### Exercise: Read the dataset \"data_4d.csv\" and make a 4d plot by each class given (‚òÖ‚òÖ‚òÖ) \n",
    "That is, make the normal 3d scatter plot and make the color of the points depending of the class for the variables you prefer:\n",
    "- Red: class 0\n",
    "- Blue: class 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aec11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
